# src/llm/llm_generator.py
from typing import List, Dict, Any
from dataclasses import dataclass
from groq import Groq

from src.utils.config import get_settings
from src.utils.logger import Logger


@dataclass
class LLMConfig:
    """
    C·∫•u h√¨nh cho Large Language Model (LLM).
    """

    model_name: str
    temperature: float
    max_tokens: int
    api_key: str

    @classmethod
    def from_settings(cls) -> "LLMConfig":
        """
        T·∫°o c·∫•u h√¨nh LLM t·ª´ file settings.
        """
        settings = get_settings()
        return cls(
            model_name=settings.GROQ_MODEL_NAME,
            temperature=0.3,
            max_tokens=1024,
            api_key=settings.GROQ_API_KEY,
        )


class LLMGenerator:
    """
    Sinh c√¢u tr·∫£ l·ªùi t·ª´ LLM d·ª±a tr√™n ng·ªØ c·∫£nh ƒë∆∞·ª£c cung c·∫•p.
    """

    def __init__(self, config: LLMConfig, log_name: str = "LLMGenerator") -> None:
        self.config = config
        # Initialize client only if api_key present
        self.client = None
        if config.api_key:
            try:
                self.client = Groq(api_key=config.api_key)
            except Exception:
                self.client = None
        self.logger = Logger(name=log_name).get_logger()
        self.logger = Logger(name=log_name).get_logger()

    # ==========================================================
    # üîπ X√¢y d·ª±ng prompt
    # ==========================================================
    def build_prompt(self, query: str, contexts: List[Dict[str, Any]]) -> str:
        """
        T·∫°o prompt ƒë·∫ßu v√†o cho LLM d·ª±a tr√™n c√¢u h·ªèi v√† c√°c ƒëo·∫°n ng·ªØ c·∫£nh.

        Args:
            query (str): C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
            contexts (List[Dict]): Danh s√°ch ng·ªØ c·∫£nh t·ª´ Qdrant (c√≥ payload.text).

        Returns:
            str: Prompt ho√†n ch·ªânh g·ª≠i l√™n LLM.
        """
        context_text = "\n\n".join(
            [
                c["payload"]["text"]
                for c in contexts
                if isinstance(c, dict) and "payload" in c and "text" in c["payload"]
            ]
        )

        prompt = f"""
B·∫°n l√† m·ªôt tr·ª£ l√Ω AI tr·∫£ l·ªùi c√¢u h·ªèi.
Nhi·ªám v·ª• c·ªßa b·∫°n l√† ƒë·ªçc k·ªπ b·ªëi c·∫£nh ƒë∆∞·ª£c cung c·∫•p d∆∞·ªõi ƒë√¢y v√† tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng m·ªôt c√°ch ch√≠nh x√°c v√† ng·∫Øn g·ªçn.

**QUY T·∫ÆC B·∫ÆT BU·ªòC:**
1. CH·ªà ƒë∆∞·ª£c ph√©p s·ª≠ d·ª•ng th√¥ng tin t·ª´ m·ª•c [B·ªëi c·∫£nh t·ª´ t√†i li·ªáu] ƒë·ªÉ h√¨nh th√†nh c√¢u tr·∫£ l·ªùi.
2. KH√îNG ƒë∆∞·ª£c s·ª≠ d·ª•ng ki·∫øn th·ª©c b√™n ngo√†i ho·∫∑c th√¥ng tin c√≥ s·∫µn trong m√¥ h√¨nh c·ªßa b·∫°n.
3. N·∫øu c√¢u tr·∫£ l·ªùi kh√¥ng c√≥ trong b·ªëi c·∫£nh, h√£y tr·∫£ l·ªùi th·∫≥ng th·∫Øn l√†: "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p."

[B·ªëi c·∫£nh t·ª´ t√†i li·ªáu]:
{context_text}

[C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng]:
{query}

[C√¢u tr·∫£ l·ªùi c·ªßa b·∫°n]:
"""
        return prompt.strip()

    # ==========================================================
    # üîπ Sinh c√¢u tr·∫£ l·ªùi
    # ==========================================================
    def generate_answer(
        self, query: str, contexts: List[Dict[str, Any]], debug: bool = False
    ) -> Dict[str, Any]:
        """
        G·ªçi LLM ƒë·ªÉ sinh c√¢u tr·∫£ l·ªùi t·ª´ truy v·∫•n v√† ng·ªØ c·∫£nh.

        Args:
            query (str): C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng.
            contexts (List[Dict]): Danh s√°ch ng·ªØ c·∫£nh (t·ª´ Qdrant search).
            debug (bool, optional): In prompt ƒë·ªÉ debug. M·∫∑c ƒë·ªãnh False.

        Returns:
            Dict[str, Any]: K·∫øt qu·∫£ g·ªìm query, answer v√† context_used.
        """
        prompt = self.build_prompt(query, contexts)

        if debug:
            self.logger.info(f"üß† Prompt g·ª≠i l√™n LLM:\n{prompt}\n")

        try:
            if not self.client:
                raise RuntimeError(
                    "LLM client not initialized (missing or invalid API key)"
                )

            response = self.client.chat.completions.create(
                model=self.config.model_name,
                messages=[
                    {
                        "role": "system",
                        "content": "B·∫°n l√† tr·ª£ l√Ω AI th√¥ng minh v√† l·ªãch s·ª±.",
                    },
                    {"role": "user", "content": prompt},
                ],
                temperature=self.config.temperature,
                max_tokens=self.config.max_tokens,
            )
            answer = response.choices[0].message.content.strip()
            self.logger.info("‚úÖ LLM tr·∫£ l·ªùi th√†nh c√¥ng.")
        except Exception as e:
            self.logger.exception("‚ùå L·ªói khi g·ªçi LLM:")
            answer = f"[L·ªói khi g·ªçi LLM]: {str(e)}"

        # If API returned empty string or None, provide a safe fallback
        if not answer:
            self.logger.warning("‚ö†Ô∏è LLM tr·∫£ l·ªùi r·ªóng, tr·∫£ fallback message.")
            answer = "T√¥i kh√¥ng t√¨m th·∫•y th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y trong t√†i li·ªáu ƒë∆∞·ª£c cung c·∫•p."

        return {
            "query": query,
            "answer": answer,
            "context_used": contexts,
        }
