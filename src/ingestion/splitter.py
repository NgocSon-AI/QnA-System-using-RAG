from typing import List, Dict, Optional

from src.utils.logger import Logger
from src.utils.text_cleaner import TextCleaner
from src.utils.config import get_settings

try:
    import tiktoken

    _HAS_TIKTOKEN = True
except Exception:
    _HAS_TIKTOKEN = False


class TextSplitter:
    """
    Chia text th√†nh c√°c ƒëo·∫°n (chunk) nh·ªè ƒë·ªÉ x·ª≠ l√Ω embedding ho·∫∑c l∆∞u v√†o Vector DB.
    """

    def __init__(
        self,
        chunk_size: int,
        chunk_overlap: int,
        model_name: str,
        log_name: str = "TextSplitter",
    ):
        """
        Args:
            chunk_size (int): S·ªë l∆∞·ª£ng token t·ªëi ƒëa m·ªói chunk.
            chunk_overlap (int): S·ªë token ch·ªìng gi·ªØa c√°c chunk.
            model_name (str): T√™n model ƒë·ªÉ ch·ªçn tokenizer t∆∞∆°ng ·ª©ng.
            log_name (str): T√™n logger, m·∫∑c ƒë·ªãnh l√† "TextSplitter".
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.logger = Logger(name=log_name).get_logger()

        if _HAS_TIKTOKEN:
            try:
                self.encoding = tiktoken.encoding_for_model(model_name)
                self.logger.info(f"‚úÖ S·ª≠ d·ª•ng tokenizer c·ªßa model: {model_name}")
            except Exception:
                self.logger.warning(
                    f"‚ö†Ô∏è Model `{model_name}` kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£, d√πng `cl100k_base` m·∫∑c ƒë·ªãnh."
                )
                self.encoding = tiktoken.get_encoding("cl100k_base")
        else:
            # Fallback: naive whitespace-based chunking if tiktoken not available
            self.encoding = None
            self.logger.warning(
                "‚ö†Ô∏è tiktoken kh√¥ng c√≥ s·∫µn, s·∫Ω d√πng fallback chia theo k√Ω t·ª± (approx)."
            )

    def split_text(
        self, text: str, metadata: Optional[Dict[str, str]] = None
    ) -> List[Dict[str, str]]:
        """
        Chia text th√†nh c√°c chunk k√®m metadata.

        Args:
            text (str): VƒÉn b·∫£n c·∫ßn chia.
            metadata (dict): Th√¥ng tin k√®m theo (VD: source, page).

        Returns:
            List[Dict[str, str]]: Danh s√°ch chunk c√≥ payload.
        """
        if not text:
            self.logger.warning("‚ö†Ô∏è Text r·ªóng, b·ªè qua.")
            return []

        chunks = []
        if self.encoding is not None:
            tokens = self.encoding.encode(text)
            start = 0
            while start < len(tokens):
                end = min(start + self.chunk_size, len(tokens))
                chunk_tokens = tokens[start:end]
                chunk_text = self.encoding.decode(chunk_tokens)
                payload = {"text": chunk_text}
                if metadata:
                    payload.update(metadata)
                chunks.append(payload)
                start += self.chunk_size - self.chunk_overlap
        else:
            # Rough character-based fallback (approximate token size)
            approx_char_size = int(self.chunk_size * 4)  # heuristic
            start = 0
            text_len = len(text)
            while start < text_len:
                end = min(start + approx_char_size, text_len)
                chunk_text = text[start:end]
                payload = {"text": chunk_text}
                if metadata:
                    payload.update(metadata)
                chunks.append(payload)
                start += approx_char_size - int(self.chunk_overlap * 4)

        self.logger.debug(f"üìÑ Chia ƒë∆∞·ª£c {len(chunks)} chunk cho 1 ƒëo·∫°n text.")
        return chunks

    def split_pages(
        self, pages: List[str], source_name: Optional[str] = None
    ) -> List[Dict[str, str]]:
        """
        Chia nhi·ªÅu trang PDF th√†nh c√°c chunk, m·ªói chunk k√®m metadata {text, source, page}.

        Args:
            pages (List[str]): Danh s√°ch c√°c trang vƒÉn b·∫£n.
            source_name (str): T√™n ngu·ªìn ho·∫∑c file PDF.

        Returns:
            List[Dict[str, str]]: Danh s√°ch t·∫•t c·∫£ c√°c chunk.
        """
        all_chunks = []
        for i, page_text in enumerate(pages):
            metadata = {"source": source_name, "page": i + 1}
            page_chunks = self.split_text(page_text, metadata)
            all_chunks.extend(page_chunks)

        self.logger.info(f"‚úÖ T·ªïng s·ªë chunk sau khi chia: {len(all_chunks)}")
        return all_chunks


if __name__ == "__main__":
    settings = get_settings()
    splitter = TextSplitter(
        chunk_size=settings.CHUNK_SIZE,
        chunk_overlap=settings.CHUNK_OVERLAP,
        model_name=settings.MODEL_TOKEN_NAME,
    )

    text = "AI l√† vi·∫øt t·∫Øt c·ªßa Artificial Intelligence. ƒê√¢y l√† m·ªôt lƒ©nh v·ª±c c·ªßa khoa h·ªçc m√°y t√≠nh..."
    chunks = splitter.split_text(text, metadata={"source": "B√°o c√°o Image Caption"})

    for i, c in enumerate(chunks, 1):
        print(f"Chunk {i}: {c['text'][:60]}...\n")
