# app/main.py
import streamlit as st
from qdrant_client import QdrantClient

from src.vector_db.search_strategy import QdrantSearcher
from src.vector_db.client import QdrantIngestor
from src.embedding.embedding import ModelEmbeddings
from src.llm.llm import LLMConfig, LLMGenerator
from src.utils.logger import Logger
from src.utils.config import get_settings
from src.utils.text_cleaner import TextCleaner

# ==============================
# üîπ Init logger
# ==============================
logger = Logger(name="STREAMLIT_APP").get_logger()
settings = get_settings()
client = QdrantClient(host="localhost", port=6333)  # ho·∫∑c "127.0.0.1"

# 2Ô∏è‚É£ Kh·ªüi t·∫°o embedding model
embedding_model = ModelEmbeddings(
    model_name="Alibaba-NLP/gte-multilingual-base",
    task="retrieval.passage",
    device="cpu",
    log_name="EMBEDDING",
)

# 3Ô∏è‚É£ Kh·ªüi t·∫°o Qdrant ingestor
qdrant_ingestor = QdrantIngestor(
    client=client,
    collection_name=settings.COLLECTION_NAME,
    vector_size=settings.VECTOR_SIZE,
    device=settings.DEVICE,
    log_name="QdrantIngestor",
    reset_collection=False,
)

# 4Ô∏è‚É£ TextCleaner
text_cleaner = TextCleaner("TextCleaner")

# 5Ô∏è‚É£ Searcher
searcher = QdrantSearcher(
    embedding_model=embedding_model,
    qdrant_db=qdrant_ingestor,
    collection_name=settings.COLLECTION_NAME,
    text_cleaner=text_cleaner,
    log_name="SearcherDemo",
)


# ==============================
# üîπ Init Searcher & LLM
# ==============================
@st.cache_resource(show_spinner=False)
def get_searcher():
    return searcher


@st.cache_resource(show_spinner=False)
def get_llm():
    config = LLMConfig.from_settings()
    return LLMGenerator(config=config)


searcher = get_searcher()
llm = get_llm()

# ==============================
# üîπ Streamlit UI
# ==============================
st.set_page_config(page_title="RAG QA App", page_icon="ü§ñ", layout="wide")
st.title("RAG QA System üß†")

st.markdown(
    "Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n ƒë·ªÉ RAG pipeline t√¨m ki·∫øm th√¥ng tin trong vector DB "
    "v√† t·∫°o c√¢u tr·∫£ l·ªùi b·∫±ng LLM."
)

# Input user query
user_query = st.text_input("Nh·∫≠p c√¢u h·ªèi:", "")

top_k = st.slider("S·ªë k·∫øt qu·∫£ retriever:", min_value=1, max_value=10, value=3)
use_hybrid = st.checkbox("S·ª≠ d·ª•ng hybrid search", value=True)

if st.button("G·ª≠i c√¢u h·ªèi") and user_query.strip():
    with st.spinner("üîç Retrieving context..."):
        try:
            if use_hybrid:
                contexts = searcher.hybrid_search(
                    query=user_query, top_k=top_k, alpha=0.9
                )
            else:
                contexts = searcher.semantic_search(query=user_query, top_k=top_k)

            if not contexts:
                st.warning("Kh√¥ng t√¨m th·∫•y th√¥ng tin ph√π h·ª£p trong t√†i li·ªáu!")
            else:
                st.success(f"‚úÖ T√¨m th·∫•y {len(contexts)} context(s).")

        except Exception as e:
            st.error(f"‚ùå L·ªói khi search: {e}")
            contexts = []

    if contexts:
        with st.spinner("üí¨ Generating answer..."):
            response = llm.generate_answer(
                query=user_query, contexts=contexts, debug=True
            )

            # Hi·ªÉn th·ªã c√¢u tr·∫£ l·ªùi
            st.markdown("### üß† C√¢u tr·∫£ l·ªùi:")
            st.info(response["answer"])

            # Hi·ªÉn th·ªã c√°c ng·ªØ c·∫£nh ƒë∆∞·ª£c d√πng
            st.markdown("### üìö C√°c ng·ªØ c·∫£nh ƒë∆∞·ª£c s·ª≠ d·ª•ng:")

            for i, c in enumerate(contexts, start=1):
                text = c.get("payload", {}).get("text", "Kh√¥ng c√≥ n·ªôi dung")
                score = c.get("score", None)

                # Ti√™u ƒë·ªÅ c·ªßa t·ª´ng ng·ªØ c·∫£nh
                exp_title = f"Context {i}"
                if score is not None:
                    exp_title += f" ‚Äî üî¢ Score: {score:.4f}"

                # D√πng expander ƒë·ªÉ ·∫©n/hi·ªán chi ti·∫øt ng·ªØ c·∫£nh
                with st.expander(exp_title):
                    st.write(text)
